\begin{tabular}{lrrrl}
\toprule
 & sc & %pl & qs & o/g \\
models &  &  &  &  \\
\midrule
o1-preview-2024-09-12 & 73.63 & 95.74 & 76.91 & ow \\
gpt-4-turbo-2024-04-09 & 58.30 & 94.88 & 61.45 & $$ \\
claude-3-5-sonnet-20240620 & 57.08 & 89.64 & 63.68 & $$ \\
gpt-4-0125-preview & 52.50 & 94.92 & 55.31 & $$ \\
llama-3.1-405b-instruct-turbo & 52.11 & 90.12 & 57.82 & ow \\
gpt-4-1106-preview & 51.99 & 98.10 & 53.00 & $$ \\
gpt-4-0613 & 51.09 & 94.88 & 53.85 & $$ \\
gpt-4o-2024-05-13 & 48.34 & 85.71 & 56.40 & $$ \\
gpt-4o-2024-08-06 & 47.71 & 85.71 & 55.66 & $$ \\
mistral-large-instruct-2407 & 45.39 & 82.21 & 55.21 & ow \\
claude-3-opus-20240229 & 42.42 & 83.10 & 51.05 & $$ \\
gemini-1.5-pro-latest & 41.90 & 81.29 & 51.55 & $$ \\
llama-3.1-70b-instruct & 38.83 & 82.14 & 47.27 & ow \\
llama-3-70b-instruct & 35.11 & 80.72 & 43.50 & ow \\
gpt-4o-mini-2024-07-18 & 34.64 & 85.06 & 40.73 & ow \\
claude-2.1 & 32.50 & 82.14 & 39.57 & $$ \\
gemini-1.5-flash-latest & 32.00 & 76.14 & 42.03 & $$ \\
claude-3-sonnet-20240229 & 30.53 & 85.24 & 35.82 & $$ \\
qwen1.5-72b-chat & 30.37 & 80.05 & 37.94 & ow \\
qwen2-72b-instruct & 30.03 & 74.52 & 40.30 & ow \\
mistral-large-2402 & 28.17 & 66.86 & 42.14 & ow \\
qwen2.5-coder-32b-instruct & 27.57 & 66.67 & 41.36 & ow \\
gemma-2-9b-it & 27.34 & 75.48 & 36.22 & ow \\
gpt-3.5-turbo-0125 & 27.22 & 89.67 & 30.36 & $$ \\
gemini-1.0-pro & 26.95 & 80.14 & 33.63 & $$ \\
command-r-plus & 24.94 & 74.90 & 33.30 & ow \\
openchat_3.5 & 23.64 & 63.52 & 37.22 & ow \\
claude-3-haiku-20240307 & 22.49 & 79.52 & 28.28 & $$ \\
sheep-duck-llama-2-70b-v1.1 & 21.50 & 41.19 & 52.20 & ow \\
llama-3-8b-instruct & 19.99 & 76.10 & 26.27 & ow \\
llama-3.1-8b-instruct & 18.36 & 72.91 & 25.18 & ow \\
openchat-3.5-1210 & 18.22 & 51.19 & 35.60 & ow \\
wizardlm-70b-v1.0 & 17.40 & 46.19 & 37.66 & ow \\
openchat-3.5-0106 & 17.10 & 52.57 & 32.52 & ow \\
qwen1.5-14b-chat & 16.80 & 40.95 & 41.02 & ow \\
mistral-medium-2312 & 16.43 & 49.25 & 33.36 & ow \\
qwen1.5-32b-chat & 15.41 & 63.69 & 24.19 & ow \\
codegemma-7b-it & 15.30 & 51.95 & 29.45 & ow \\
dolphin-2.5-mixtral-8x7b & 15.10 & 46.38 & 32.55 & ow \\
codellama-34b-instruct & 14.35 & 33.57 & 42.76 & ow \\
command-r & 14.15 & 61.67 & 22.95 & ow \\
gemma-1.1-7b-it & 14.14 & 49.67 & 28.46 & ow \\
sus-chat-34b & 14.11 & 54.40 & 25.93 & ow \\
aya-23-35b & 13.35 & 47.90 & 27.88 & ow \\
mixtral-8x22b-instruct-v0.1 & 12.69 & 52.14 & 24.33 & ow \\
tulu-2-dpo-70b & 12.62 & 49.76 & 25.37 & ow \\
nous-hermes-2-mixtral-8x7b-sft & 11.95 & 39.68 & 30.12 & ow \\
aya-23-8b & 11.72 & 45.24 & 25.90 & ow \\
wizardlm-13b-v1.2 & 11.48 & 39.57 & 29.00 & ow \\
vicuna-33b-v1.3 & 11.27 & 23.81 & 47.32 & ow \\
llama-3.1-nemotron-70b-instruct & 10.16 & 20.76 & 48.92 & ow \\
mistral-7b-instruct-v0.2 & 9.75 & 36.91 & 26.42 & ow \\
yi-34b-chat & 8.27 & 40.86 & 20.25 & ow \\
mixtral-8x7b-instruct-v0.1 & 8.17 & 47.62 & 17.15 & ow \\
mistral-7b-instruct-v0.1 & 8.01 & 37.14 & 21.58 & ow \\
yi-1.5-34b-chat & 7.67 & 52.38 & 14.65 & ow \\
vicuna-13b-v1.5 & 7.01 & 39.52 & 17.73 & ow \\
yi-1.5-6b-chat & 6.73 & 41.43 & 16.25 & ow \\
starling-lm-7b-beta & 6.56 & 30.89 & 21.25 & ow \\
phi-3-mini-128k-instruct & 6.33 & 34.52 & 18.34 & ow \\
qwen2-7b-instruct & 6.18 & 35.32 & 17.51 & ow \\
salamandra-7b-instruct & 6.04 & 21.75 & 27.78 & ow \\
sheep-duck-llama-2-13b & 5.39 & 31.90 & 16.90 & ow \\
yi-1.5-9b-chat & 4.37 & 38.10 & 11.48 & ow \\
gemma-2-27b-it & 3.51 & 11.90 & 29.51 & ow \\
gemma-1.1-2b-it & 2.91 & 22.62 & 12.87 & ow \\
gemma-2-2b-it & 2.67 & 38.33 & 6.96 & ow \\
qwen1.5-7b-chat & 2.58 & 30.24 & 8.53 & ow \\
gemma-7b-it & 1.82 & 17.78 & 10.23 & ow \\
llama-2-70b-chat & 0.81 & 7.14 & 11.31 & ow \\
qwen1.5-0.5b-chat & 0.12 & 25.72 & 0.48 & ow \\
qwen1.5-1.8b-chat & 0.00 & 15.24 & 0.00 & ow \\
\bottomrule
\end{tabular}
