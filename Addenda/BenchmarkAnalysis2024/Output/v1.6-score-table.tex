\begin{tabular}{lrl}
\toprule
 & sc & o/g \\
models &  &  \\
\midrule
o1-preview-2024-09-12 & 73.63 & ow \\
gpt-4-turbo-2024-04-09 & 58.30 & $$ \\
claude-3-5-sonnet-20240620 & 57.08 & $$ \\
gpt-4-0125-preview & 52.50 & $$ \\
llama-3.1-405b-instruct-turbo & 52.11 & ow \\
gpt-4-1106-preview & 51.99 & $$ \\
gpt-4-0613 & 51.09 & $$ \\
gpt-4o-2024-05-13 & 48.34 & $$ \\
gpt-4o-2024-08-06 & 47.71 & $$ \\
mistral-large-instruct-2407 & 45.39 & ow \\
claude-3-opus-20240229 & 42.42 & $$ \\
gemini-1.5-pro-latest & 41.90 & $$ \\
llama-3.1-70b-instruct & 38.83 & ow \\
llama-3-70b-instruct & 35.11 & ow \\
gpt-4o-mini-2024-07-18 & 34.64 & ow \\
claude-2.1 & 32.50 & $$ \\
gemini-1.5-flash-latest & 32.00 & $$ \\
claude-3-sonnet-20240229 & 30.53 & $$ \\
qwen1.5-72b-chat & 30.37 & ow \\
qwen2-72b-instruct & 30.03 & ow \\
mistral-large-2402 & 28.17 & ow \\
qwen2.5-coder-32b-instruct & 27.57 & ow \\
gemma-2-9b-it & 27.34 & ow \\
gpt-3.5-turbo-0125 & 27.22 & $$ \\
gemini-1.0-pro & 26.95 & $$ \\
command-r-plus & 24.94 & ow \\
openchat_3.5 & 23.64 & ow \\
claude-3-haiku-20240307 & 22.49 & $$ \\
sheep-duck-llama-2-70b-v1.1 & 21.50 & ow \\
llama-3-8b-instruct & 19.99 & ow \\
llama-3.1-8b-instruct & 18.36 & ow \\
openchat-3.5-1210 & 18.22 & ow \\
wizardlm-70b-v1.0 & 17.40 & ow \\
openchat-3.5-0106 & 17.10 & ow \\
qwen1.5-14b-chat & 16.80 & ow \\
mistral-medium-2312 & 16.43 & ow \\
\bottomrule
\end{tabular}
